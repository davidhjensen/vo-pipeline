{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bbe16fc",
   "metadata": {},
   "source": [
    "# VO Pipeline\n",
    "_Vision Algorithms for Mobile Robotics | Fall 2025_<br><br>\n",
    "_David Jensen, Alessandro Pirini, Matteo Rubini, Alessandro Ferranti_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c46f58",
   "metadata": {},
   "source": [
    "## Notes on writing code\n",
    "For now, try to make each block a function; see below for format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_is_a_function(state, params, whatever_else_function_needs):\n",
    "    # update state in place and return only things that not included in the state\n",
    "    return None\n",
    "\n",
    "# you can then call your function below it to debug, process data for the next step\n",
    "this_is_a_function(None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f7f4a",
   "metadata": {},
   "source": [
    "This way debugging still works easily, but then we can have a couple main blocks at the very end that handle everything nicely.<br>\n",
    "For the blocks in the _Operation_ section, probably pass in the state dictionary and parameter class and then whatever else might be relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fadbe",
   "metadata": {},
   "source": [
    "Also, when you need to add a parameter (ex confidence for RANSAC), you have options. If it is a global paramter that is the same for all datasets, add it [here](#paramaters-for-all-datasets). If it could change based on the dataset, add it [here](#paramaters-for-specific-datasets). After adding the parameter value in the appropriant place, add where required [here](#paramaters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16a4b0",
   "metadata": {},
   "source": [
    "Another cool thing is `Jupyter Variables`: click on it in the top toolbar, and it shows name, type, size, and value for all variables. Nice for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c5494",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787b350",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18998a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import skimage\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839c4c9",
   "metadata": {},
   "source": [
    "### Data\n",
    "_Ensure that all datasets have been downloaded and unzipped into their respective folders_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3722f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset -> 0: KITTI, 1: Malaga, 2: Parking, 3: Own Dataset\n",
    "DATASET = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa422c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "# (Set these variables before running)\n",
    "kitti_path = \"kitti/kitti05/kitti\"\n",
    "malaga_path = \"malaga/malaga-urban-dataset-extract-07\"\n",
    "parking_path = \"parking/parking\"\n",
    "# own_dataset_path = \"/path/to/own_dataset\"\n",
    "\n",
    "if DATASET == 0:\n",
    "    assert 'kitti_path' in locals(), \"You must define kitti_path\"\n",
    "    img_dir = os.path.join(kitti_path, '05/image_0')\n",
    "    images = glob(os.path.join(img_dir, '*.png'))\n",
    "    last_frame = 4540\n",
    "    K = np.array([\n",
    "        [7.18856e+02, 0, 6.071928e+02],\n",
    "        [0, 7.18856e+02, 1.852157e+02],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    ground_truth = np.loadtxt(os.path.join(kitti_path, 'poses', '05.txt'))\n",
    "    ground_truth = ground_truth[:, [-9, -1]]  # same as MATLAB(:, [end-8 end])\n",
    "elif DATASET == 1:\n",
    "    assert 'malaga_path' in locals(), \"You must define malaga_path\"\n",
    "    img_dir = os.path.join(malaga_path, 'malaga-urban-dataset-extract-07_rectified_800x600_Images')\n",
    "    images = sorted(glob(os.path.join(img_dir, '*.png')))\n",
    "    last_frame = len(images)\n",
    "    K = np.array([\n",
    "        [621.18428, 0, 404.0076],\n",
    "        [0, 621.18428, 309.05989],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "elif DATASET == 2:\n",
    "    assert 'parking_path' in locals(), \"You must define parking_path\"\n",
    "    img_dir = os.path.join(kitti_path, '05/image_0')\n",
    "    images = glob(os.path.join(img_dir, '*.png'))\n",
    "    last_frame = 598\n",
    "    K = np.loadtxt(os.path.join(parking_path, 'K.txt'), delimiter=\",\", usecols=(0, 1, 2))\n",
    "    ground_truth = np.loadtxt(os.path.join(parking_path, 'poses.txt'))\n",
    "    ground_truth = ground_truth[:, [-9, -1]]\n",
    "elif DATASET == 3:\n",
    "    # Own Dataset\n",
    "    # TODO: define your own dataset and load K obtained from calibration of own camera\n",
    "    assert 'own_dataset_path' in locals(), \"You must define own_dataset_path\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cdc90",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4c0a3",
   "metadata": {},
   "source": [
    "### Paramaters for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ebdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramaters for Shi-Tomasi corners\n",
    "feature_params = dict( maxCorners = 10,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Parameters for LKT\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da08b2",
   "metadata": {},
   "source": [
    "### Paramaters for specific datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next keyframe to use for bootstrapping\n",
    "KITTI_BS_KF = 3\n",
    "MALAGA_BS_KF = 5\n",
    "PARKING_BS_KF = 5\n",
    "CUSTOM_BS_KF = 5\n",
    "\n",
    "# Number of rows and columns to divide image into for feature detection and number of features to track in each cell\n",
    "KITTI_ST_ROWS, KITTI_ST_COLS, KITTI_NUM_FEATURES = 2, 4, 20\n",
    "MALAGA_ST_ROWS, MALAGA_ST_COLS, MALAGA_NUM_FEATURES = 2, 4, 20\n",
    "PARKING_ST_ROWS, PARKING_ST_COLS, PARKING_NUM_FEATURES = 2, 4, 20\n",
    "CUSTOM_ST_ROWS, CUSTOM_ST_COLS, CUSTOM_NUM_FEATURES = 2, 4, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b09283",
   "metadata": {},
   "source": [
    "### Instantiate params class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63624629",
   "metadata": {},
   "source": [
    "#### Generate masks for feature tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_masks(img_path, rows, cols) -> list[np.ndarray]:\n",
    "    # get image shape\n",
    "    img = cv2.imread(img_path)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    # get boundries of the cells\n",
    "    row_boundries = np.linspace(0, H, rows + 1, dtype=int)\n",
    "    col_boundries = np.linspace(0, W, cols + 1, dtype=int)\n",
    "\n",
    "    # create masks left to right, top to bottom\n",
    "    masks = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            mask = np.zeros((H, W), dtype=\"uint8\")\n",
    "            r_s, r_e = row_boundries[[row, row + 1]]\n",
    "            c_s, c_e = col_boundries[[col, col + 1]]\n",
    "            mask[r_s:r_e, c_s:c_e] = 255\n",
    "            masks.append(mask)\n",
    "            \n",
    "            # visulaization\n",
    "            # vis = np.zeros_like(img)\n",
    "            # vis[mask] = img[mask]\n",
    "            # cv2.imshow(\"masked\", vis)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736dfd9",
   "metadata": {},
   "source": [
    "#### Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VO_Params():\n",
    "    bs_kf_1: str # path to first keyframe used for bootstrapping dataset\n",
    "    bs_kf_2: str # path to second keyframe used for bootstrapping dataset\n",
    "    feature_masks: list[np.ndarray] # mask image into regions for feature tracking \n",
    "    shi_tomasi_params: dict\n",
    "    klt_params: dict\n",
    "    k: np.ndarray # camera intrinsics matrix\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "    def __init__(self, bs_kf_1, bs_kf_2, feature_masks, shi_tomasi_params, klt_params, k):\n",
    "        self.bs_kf_1 = bs_kf_1\n",
    "        self.bs_kf_2 = bs_kf_2\n",
    "        self.feature_masks = feature_masks\n",
    "        self.shi_tomasi_params = shi_tomasi_params\n",
    "        self.klt_params = klt_params\n",
    "        self.k = k\n",
    "        # ADD NEW PARAMS HERE\n",
    "\n",
    "if DATASET == 0:\n",
    "    assert 'kitti_path' in locals(), \"You must define kitti_path\"\n",
    "    bs_kf_1 = images[0]\n",
    "    bs_kf_2 = images[KITTI_BS_KF]\n",
    "    feature_masks = get_feature_masks(bs_kf_1, KITTI_ST_ROWS, KITTI_ST_COLS)\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "elif DATASET == 1:\n",
    "    assert 'malaga_path' in locals(), \"You must define malaga_path\"\n",
    "    bs_kf_1 = images[0]\n",
    "    bs_kf_2 = images[MALAGA_BS_KF]\n",
    "    feature_masks = get_feature_masks(bs_kf_1, MALAGA_ST_ROWS, MALAGA_ST_COLS)\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "elif DATASET == 2:\n",
    "    assert 'parking_path' in locals(), \"You must define parking_path\"\n",
    "    bs_kf_1 = images[0]\n",
    "    bs_kf_2 = images[PARKING_BS_KF]\n",
    "    feature_masks = get_feature_masks(bs_kf_1, PARKING_ST_ROWS, PARKING_ST_COLS)\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "elif DATASET == 3:\n",
    "    # Own Dataset\n",
    "    # TODO: define your own dataset and load K obtained from calibration of own camera\n",
    "    assert 'own_dataset_path' in locals(), \"You must define own_dataset_path\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset index\")\n",
    "\n",
    "# ADD NEW PARAMS HERE TO THE INIT\n",
    "params = VO_Params(bs_kf_1, bs_kf_2, feature_masks, feature_params, lk_params, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0dc41",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "- Select two keyframes with large enough baseline\n",
    "- Use indirect (feature-based) or direct (KLT) method to establish keypoint corrispondences between frames\n",
    "- Estimate relative pose and triangulate points to bootstrap point cloud (5-pt RANSAC)\n",
    "- Initialize VO pipeline with inlier keypoints and their associated landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861e643",
   "metadata": {},
   "source": [
    "### Initialization,Feature extraction\n",
    "\n",
    "Detect a set of 2D keypoints in the first bootstrap keyframe (bs_kf_1) that are well distributed across the image.\n",
    "\n",
    "Implementation:\n",
    "- We use `cv2.goodFeaturesToTrack` (Shi–Tomasi) to detect corners.\n",
    "- To avoid clustering of keypoints in high-texture areas, we apply Shi–Tomasi separately in multiple image regions using `params.feature_masks` and then concatenate the results.\n",
    "\n",
    "Output:\n",
    "- `st_corners`: array of shape `(N, 1, 2)` (float32), suitable for KLT tracking with `cv2.calcOpticalFlowPyrLK`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img_grayscale, params):\n",
    "    \"\"\"\n",
    "    Step 1 (Initialization): detect Shi–Tomasi corners on a grid using feature masks.\n",
    "\n",
    "    Args:\n",
    "        img_grayscale (np.ndarray): bootstrap keyframe 1 in grayscale (H x W).\n",
    "        params (VO_Params): contains feature_masks and shi_tomasi_params.\n",
    "\n",
    "    Returns:\n",
    "        st_corners (np.ndarray): (N, 1, 2) float32 corners for KLT tracking.\n",
    "    \"\"\"\n",
    "    st_corners = np.empty((0, 1, 2), dtype=np.float32)\n",
    "    for n, mask in enumerate(params.feature_masks):\n",
    "        features = cv2.goodFeaturesToTrack(img_grayscale, mask=mask, **params.shi_tomasi_params)\n",
    "        # If no corners are found in this region, skip it\n",
    "        if features is None: \n",
    "            print(f\"No features found for mask {n+1}!\")\n",
    "            continue\n",
    "        # Warn if very few features were found in this region (not necessarily an error)\n",
    "        if features.shape[0] < 10:\n",
    "            print(f\"Only {features.shape[0]} features found for mask {n+1}!\")\n",
    "        st_corners = np.vstack((st_corners, features))\n",
    "    return st_corners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d689b0",
   "metadata": {},
   "source": [
    "### Initialization, KLT tracking across intermediate frames\n",
    "\n",
    "Establish 2D–2D correspondences between the two bootstrap keyframes (bs_kf_1 → bs_kf_2).\n",
    "\n",
    "Implementation:\n",
    "- Tracking the detected keypoints using KLT optical flow (`cv2.calcOpticalFlowPyrLK`).\n",
    "- Instead of tracking directly from keyframe 1 to keyframe 2 in one shot, we track *frame-by-frame* across the intermediate frames,more stable when the motion between keyframes is larger.\n",
    "- We maintain a boolean mask `still_detected` that keeps track of keypoints successfully tracked at every frame.\n",
    "\n",
    "Output:\n",
    "- `points[still_detected]`: tracked keypoints in `bs_kf_2`\n",
    "- `initial_points`: initial deteced keypoints in `bs_kf_1`\n",
    "- `still_detected`: boolean mask (relative to the original set) indicating which keypoints survived the entire bootstrap tracking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c38127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tracking_klt(images,params,st_corners_kf_1):\n",
    "    \"\"\"\n",
    "    Module 2-Initialization: track keypoints from bs_kf_1 to bs_kf_2 with KLT across intermediate frames.\n",
    "\n",
    "    Args:\n",
    "        images (list[str]): full dataset sequence (paths).\n",
    "        params (VO_Params): contains bs_kf_1, bs_kf_2, klt_params.\n",
    "        st_corners_kf_1 (np.ndarray): (N,1,2) keypoints in bs_kf_1.\n",
    "\n",
    "    Returns:\n",
    "        points[still_detected] (np.ndarray): (M,1,2) tracked points in bs_kf_2.\n",
    "        initial_points (np.ndarray): (N,1,2) points in bs_kf_1.\n",
    "        still_detected (np.ndarray): (N,) boolean mask of points visible i all frames.\n",
    "    \"\"\"\n",
    "    img_bs_kf_1_index=images.index(params.bs_kf_1)\n",
    "    img_bs_kf_2_index=images.index(params.bs_kf_2)\n",
    "    still_detected=np.ones(st_corners_kf_1.shape[0],dtype=bool)\n",
    "    points = st_corners_kf_1.copy()\n",
    "    initial_points = st_corners_kf_1.copy()\n",
    "    for i in range(img_bs_kf_1_index, img_bs_kf_2_index):\n",
    "        current_image=cv2.imread(images[i],cv2.IMREAD_GRAYSCALE)\n",
    "        next_image=cv2.imread(images[i+1],cv2.IMREAD_GRAYSCALE)\n",
    "        nextPts,status,error=cv2.calcOpticalFlowPyrLK(current_image,next_image,points, None, **params.klt_params)\n",
    "        points=nextPts\n",
    "        status=status.flatten()\n",
    "        still_detected=still_detected & (status==1)\n",
    "\n",
    "    return points[still_detected], initial_points, still_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031091c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check point 1 and point 2\n",
    "#read bootstrap keyframes\n",
    "img_bs_kf_1 = cv2.imread(params.bs_kf_1,cv2.IMREAD_GRAYSCALE)\n",
    "img_bs_kf_2 = cv2.imread(params.bs_kf_2,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#1)extract features\n",
    "st_corners_kf_1 = extract_features(img_bs_kf_1, params)\n",
    "\n",
    "#2)KLT interframe tracking\n",
    "st_corners_kf_2, st_corners_kf_1,still_detected =feature_tracking_klt(images, params, st_corners_kf_1)\n",
    "\n",
    "print(\"Initial corners:\",st_corners_kf_1.shape[0])\n",
    "print(\"Tracked corners:\",st_corners_kf_2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61206a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple visualization of tracked points on first keyframe\n",
    "vis = cv2.cvtColor(img_bs_kf_1, cv2.COLOR_GRAY2BGR)\n",
    "for p in st_corners_kf_1[still_detected][:200]:\n",
    "    x,y = p.ravel()\n",
    "    cv2.circle(vis, (int(x), int(y)), 2, (0,255,0), -1)\n",
    "cv2.imshow(\"Tracked corners (alive) on kf1\", vis); cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track features to second keyframe\n",
    "st_corners_kf_2, st, err = cv2.calcOpticalFlowPyrLK(img_bs_kf_1, img_bs_kf_2, st_corners_kf_1, None, **params.klt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a8fa2",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "TODO: find transformation between the two frames using cv2.findHomography()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use ransac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07983c3e",
   "metadata": {},
   "source": [
    "### Triangulate points to get point cloud\n",
    "TODO: find 3d points using triangulatePoints(); the projection matrices are $K*[R|T]$ where $[R|T]$ is $[I|0]$ for the first image and the homography from above for the second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ae1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_bootstrap(params: VO_Params, H: np.ndarray, points_1: np.ndarray, points_2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Bootstrap the initial 3D point cloud using least squares assuming the first frame is the origin\n",
    "\n",
    "    Args:\n",
    "        params (VO_Params): params object for the dataset being used\n",
    "        H (np.ndarray): homographic transformation from bootstrap keyframe 1 to 2\n",
    "        points_1 (np.ndarray): keypoints detected in bootstrap keyframe 1\n",
    "        points_2 (np.ndarray): keypoints tracked in bootstrap keyframe 2\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: [3 x k] array of triangulated points\n",
    "    \"\"\"\n",
    "\n",
    "    # projection matrices\n",
    "    proj_1 = params.k @ np.hstack([np.eye(3), np.zeros((3,1))])\n",
    "    proj_2 = params.k @ H\n",
    "\n",
    "    # triangulate homogeneous coordinates using DLT\n",
    "    points_homo = cv2.triangulatePoints(proj_1, proj_2, points_1, points_2)\n",
    "\n",
    "    # convert back to 3D\n",
    "    points_3d = (points_homo[:3, :]/points_homo[3, :])\n",
    "\n",
    "    return points_3d\n",
    "pts = triangulate_bootstrap(params, np.hstack([np.eye(3), np.ones((3,1))]), H_bootstrap, st_corners_kf_1, st_corners_kf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08c00f",
   "metadata": {},
   "source": [
    "### Build state to initialize the algorithm\n",
    "The state is $(P^i, X^i, C^i, F^i, \\Tau^i)$ where<br>\n",
    "$P^i$ is a `[k x 1 x 2]` matrix of initial features' pixel in the second keyframe of the dataset<br>\n",
    "$X^i$ is a `[3 x k]` matrix of the 3D cooridinates of the corrisponding landmarks<br>\n",
    "$C^i$ is a `[m x 1 x 2]` matrix of current locations of candidate keypoints (empty to start so `c=0`)<br>\n",
    "$F^i$ is a `[m x 1 x 2]` matrix of initial observation of candidate keypoints (empty to start so `c=0`)<br>\n",
    "$\\Tau^i$ is a `[m x 12]` matrix of the camera pose during the initial observation<br>\n",
    "This can be stored in a dict $S = \\{P: [k \\times 1 \\times 2], X: [3 \\times k], ...\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that only P and X should be populated; the rest should be initialized as empty (ex for Ci: np.empty((0, 1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f86f87",
   "metadata": {},
   "source": [
    "# Operation\n",
    "- Match keypoints in current image to existing landmarks\n",
    "    - Extract keypoints (Harris)\n",
    "    - Track (KLT)\n",
    "- Estimate pose\n",
    "    - Estimate pose and handle outliers (P3P plus RANSAC)\n",
    "- Add new landmarks as needed by triangulating new features\n",
    "    - Keep track of candidate landmarks\n",
    "        - Keypoint itself\n",
    "        - Observation when first seen\n",
    "        - Pose when first seen\n",
    "    - Only add when they have been tracked for long enough and baselineis large enough\n",
    "    - Discard if track fails<br>\n",
    "\n",
    "_NOTE: this starts at the frame after the second keyframe (`bs_kf_2`) and goes until the last frame in the dataset_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20de2d8",
   "metadata": {},
   "source": [
    "### Track keypoints forward one frame\n",
    "TODO: Use cv2.calcOpticalFlowPyrLK() with previous frame, current frame, $P^i$ and $C^i$,  - see use in initialization for example and then update $P^i$ and $X^i$ as well as $C^i$, $F^i$ and $\\Tau^i$ based on the features that were successfully tracked (ie remove any features that were not tracked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac891bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d629eebb",
   "metadata": {},
   "source": [
    "### Estimate pose\n",
    "TODO: use cv2.solvePnPRansac() with updated $P^i$, $X^i$, $K$, to find pose of camera at current position and then updated $P^i$ and $X^i$ with the inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a7500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02901b0f",
   "metadata": {},
   "source": [
    "### Try triangulating\n",
    "TODO: Check the angle between the the first observation ($X^i$ and $\\Tau^i$) of candidate keypoints and the current observation ($C^i$ and current pose) - triangulate features using cv2.triangulatePoints() if angle is above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61945b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4a2379b",
   "metadata": {},
   "source": [
    "### Find new features\n",
    "TODO: use cv2.goodFeaturesToTrack() - see use in initialization for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f162ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tstt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f324c98",
   "metadata": {},
   "source": [
    "### Add new features\n",
    "TODO: if the feature is not already being tracked as an actual keypoint or candidate keypoint, update $C^i$ and $X^i$ with keypoint location and $\\Tau^i$ with the current camera pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints2set(keypoints: np.ndarray) -> set:\n",
    "    \"\"\"Convert numpy keypoint list [k x 1 x 2] to a set of keypoints\n",
    "\n",
    "    Args:\n",
    "        keypoints (np.ndarray): keypoint array\n",
    "\n",
    "    Returns:\n",
    "        set: keypoint set\n",
    "    \"\"\"\n",
    "    return set([(row[0][0], row[0][1]) for row in keypoints.tolist()])\n",
    "\n",
    "def set2keypoints(keypoint_set: set) -> np.ndarray:\n",
    "    \"\"\"Convert keypoint set (u, v) to a numpy array [k x 1 x 2]\n",
    "\n",
    "    Args:\n",
    "        keypoint_set (set): keypoint set\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: keypoint array with shape [k x 1 x 2]\n",
    "    \"\"\"\n",
    "    return np.array([[[keypoint[0], keypoint[1]]] for keypoint in keypoint_set])\n",
    "\n",
    "# find which features are actually new\n",
    "new_features = set2keypoints(keypoint2set(S[\"P\"]) + keypoint2set(S[\"C\"]) - keypoint2set(candidate_new_features))\n",
    "\n",
    "# append new features to current points, first observed points, and first observed camera pose\n",
    "S[\"C\"] = np.vstack((S[\"C\"], new_features))\n",
    "S[\"X\"] = np.vstack((S[\"X\"], new_features))\n",
    "S[\"T\"] = np.vstack(S[\"T\"], cur_pose.flatten()[None, :].repeat(len(new_features)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
