{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bbe16fc",
   "metadata": {},
   "source": [
    "# VO Pipeline\n",
    "_Vision Algorithms for Mobile Robotics | Fall 2025_<br><br>\n",
    "_David Jensen, Alessandro Pirini, Matteo Rubini, Alessandro Ferranti_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c46f58",
   "metadata": {},
   "source": [
    "## Notes on writing code\n",
    "For now, try to make each block a function; see below for format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def this_is_a_function(state, params, whatever_else_function_needs):\n",
    "    # update state in place and return only things that not included in the state\n",
    "    return None\n",
    "\n",
    "# you can then call your function below it to debug, process data for the next step\n",
    "this_is_a_function(None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f7f4a",
   "metadata": {},
   "source": [
    "This way debugging still works easily, but then we can have a couple main blocks at the very end that handle everything nicely.<br>\n",
    "For the blocks in the _Operation_ section, probably pass in the state dictionary and parameter class and then whatever else might be relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fadbe",
   "metadata": {},
   "source": [
    "Also, when you need to add a parameter (ex confidence for RANSAC), you have options. If it is a global paramter that is the same for all datasets, add it [here](#paramaters-for-all-datasets). If it could change based on the dataset, add it [here](#paramaters-for-specific-datasets). After adding the parameter value in the appropriant place, add where required [here](#paramaters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b16a4b0",
   "metadata": {},
   "source": [
    "Another cool thing is `Jupyter Variables`: click on it in the top toolbar, and it shows name, type, size, and value for all variables. Nice for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c5494",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787b350",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18998a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.patches import FancyArrowPatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839c4c9",
   "metadata": {},
   "source": [
    "### Data\n",
    "_Ensure that all datasets have been downloaded and unzipped into their respective folders_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3722f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset -> 0: KITTI, 1: Malaga, 2: Parking, 3: Own Dataset\n",
    "DATASET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa422c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "# (Set these variables before running)\n",
    "kitti_path = \"kitti/kitti05/kitti\"\n",
    "malaga_path = \"malaga/malaga-urban-dataset-extract-07\"\n",
    "parking_path = \"parking/parking\"\n",
    "# own_dataset_path = \"/path/to/own_dataset\"\n",
    "\n",
    "if DATASET == 0:\n",
    "    assert 'kitti_path' in locals(), \"You must define kitti_path\"\n",
    "    img_dir = os.path.join(kitti_path, '05/image_0')\n",
    "    images = glob(os.path.join(img_dir, '*.png'))\n",
    "    last_frame = 4540\n",
    "    K = np.array([\n",
    "        [7.18856e+02, 0, 6.071928e+02],\n",
    "        [0, 7.18856e+02, 1.852157e+02],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    ground_truth = np.loadtxt(os.path.join(kitti_path, 'poses', '05.txt'))\n",
    "    ground_truth = ground_truth[:, [-9, -1]]  # same as MATLAB(:, [end-8 end])\n",
    "\n",
    "elif DATASET == 1:\n",
    "    assert 'malaga_path' in locals(), \"You must define malaga_path\"\n",
    "    img_dir = os.path.join(malaga_path, 'malaga-urban-dataset-extract-07_rectified_800x600_Images')\n",
    "    images = sorted(glob(os.path.join(img_dir, '*.png')))\n",
    "    last_frame = len(images)\n",
    "    K = np.array([\n",
    "        [621.18428, 0, 404.0076],\n",
    "        [0, 621.18428, 309.05989],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    ground_truth = None\n",
    "    \n",
    "elif DATASET == 2:\n",
    "    assert 'parking_path' in locals(), \"You must define parking_path\"\n",
    "    img_dir = os.path.join(kitti_path, '05/image_0')\n",
    "    images = glob(os.path.join(img_dir, '*.png'))\n",
    "    last_frame = 598\n",
    "    K = np.loadtxt(os.path.join(parking_path, 'K.txt'), delimiter=\",\", usecols=(0, 1, 2))\n",
    "    ground_truth = np.loadtxt(os.path.join(parking_path, 'poses.txt'))\n",
    "    ground_truth = ground_truth[:, [-9, -1]]\n",
    "    \n",
    "elif DATASET == 3:\n",
    "    # Own Dataset\n",
    "    # TODO: define your own dataset and load K obtained from calibration of own camera\n",
    "    assert 'own_dataset_path' in locals(), \"You must define own_dataset_path\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cdc90",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4c0a3",
   "metadata": {},
   "source": [
    "### Paramaters for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ebdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramaters for Shi-Tomasi corners\n",
    "feature_params = dict( maxCorners = 10,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Parameters for LKT\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# min dist in pxl from a new feature to the nearest existing feature for the new feature to be added\n",
    "new_feature_min_dist = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da08b2",
   "metadata": {},
   "source": [
    "### Paramaters for specific datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Next keyframe to use for bootstrapping\n",
    "KITTI_BS_KF = 5\n",
    "MALAGA_BS_KF = 5\n",
    "PARKING_BS_KF = 5\n",
    "CUSTOM_BS_KF = 5\n",
    "\n",
    "# Number of rows and columns to divide image into for feature detection and number of features to track in each cell\n",
    "KITTI_ST_ROWS, KITTI_ST_COLS, KITTI_NUM_FEATURES = 2, 4, 20\n",
    "MALAGA_ST_ROWS, MALAGA_ST_COLS, MALAGA_NUM_FEATURES = 2, 4, 20\n",
    "PARKING_ST_ROWS, PARKING_ST_COLS, PARKING_NUM_FEATURES = 2, 4, 20\n",
    "CUSTOM_ST_ROWS, CUSTOM_ST_COLS, CUSTOM_NUM_FEATURES = 2, 4, 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b09283",
   "metadata": {},
   "source": [
    "### Instantiate params class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63624629",
   "metadata": {},
   "source": [
    "#### Generate masks for feature tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcc4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_masks(img_path, rows, cols) -> list[np.ndarray]:\n",
    "    # get image shape\n",
    "    img = cv2.imread(img_path)\n",
    "    H, W = img.shape[:2]\n",
    "\n",
    "    # get boundries of the cells\n",
    "    row_boundries = np.linspace(0, H, rows + 1, dtype=int)\n",
    "    col_boundries = np.linspace(0, W, cols + 1, dtype=int)\n",
    "\n",
    "    # create masks left to right, top to bottom\n",
    "    masks = []\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            mask = np.zeros((H, W), dtype=\"uint8\")\n",
    "            r_s, r_e = row_boundries[[row, row + 1]]\n",
    "            c_s, c_e = col_boundries[[col, col + 1]]\n",
    "            mask[r_s:r_e, c_s:c_e] = 255\n",
    "            masks.append(mask)\n",
    "            \n",
    "            # visulaization\n",
    "            # vis = np.zeros_like(img)\n",
    "            # vis[mask] = img[mask]\n",
    "            # cv2.imshow(\"masked\", vis)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736dfd9",
   "metadata": {},
   "source": [
    "#### Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VO_Params():\n",
    "    bs_kf_1: str # path to first keyframe used for bootstrapping dataset\n",
    "    bs_kf_2: str # path to second keyframe used for bootstrapping dataset\n",
    "    feature_masks: list[np.ndarray] # mask image into regions for feature tracking \n",
    "    shi_tomasi_params: dict\n",
    "    klt_params: dict\n",
    "    k: np.ndarray # camera intrinsics matrix\n",
    "    start_idx: int # index of the frame to start continous operation at (2nd bootstrap keyframe index)\n",
    "    new_feature_min_dist: float # min dist in pxl from a new feature to the nearest existing feature for the new feature to be added\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "    def __init__(self, bs_kf_1, bs_kf_2, feature_masks, shi_tomasi_params, klt_params, k, start_idx, new_feature_min_dist):\n",
    "        self.bs_kf_1 = bs_kf_1\n",
    "        self.bs_kf_2 = bs_kf_2\n",
    "        self.feature_masks = feature_masks\n",
    "        self.shi_tomasi_params = shi_tomasi_params\n",
    "        self.klt_params = klt_params\n",
    "        self.k = k\n",
    "        self.start_idx = start_idx\n",
    "        self.new_feature_min_dist = new_feature_min_dist\n",
    "        # ADD NEW PARAMS HERE\n",
    "\n",
    "if DATASET == 0:\n",
    "    assert 'kitti_path' in locals(), \"You must define kitti_path\"\n",
    "    bs_kf_1 = images[0]\n",
    "    bs_kf_2 = images[KITTI_BS_KF]\n",
    "    start_idx = KITTI_BS_KF\n",
    "    feature_masks = get_feature_masks(bs_kf_1, KITTI_ST_ROWS, KITTI_ST_COLS)\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "elif DATASET == 1:\n",
    "    assert 'malaga_path' in locals(), \"You must define malaga_path\"\n",
    "    bs_kf_1 = images[0]\n",
    "    bs_kf_2 = images[MALAGA_BS_KF]\n",
    "    start_idx = MALAGA_BS_KF\n",
    "    feature_masks = get_feature_masks(bs_kf_1, MALAGA_ST_ROWS, MALAGA_ST_COLS)\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "elif DATASET == 2:\n",
    "    assert 'parking_path' in locals(), \"You must define parking_path\"\n",
    "    bs_kf_1 = images[0]\n",
    "    bs_kf_2 = images[PARKING_BS_KF]\n",
    "    start_idx = PARKING_BS_KF\n",
    "    feature_masks = get_feature_masks(bs_kf_1, PARKING_ST_ROWS, PARKING_ST_COLS)\n",
    "    # ADD NEW PARAMS HERE\n",
    "\n",
    "elif DATASET == 3:\n",
    "    # Own Dataset\n",
    "    # TODO: define your own dataset and load K obtained from calibration of own camera\n",
    "    assert 'own_dataset_path' in locals(), \"You must define own_dataset_path\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid dataset index\")\n",
    "\n",
    "# ADD NEW PARAMS HERE TO THE INIT\n",
    "params = VO_Params(bs_kf_1, bs_kf_2, feature_masks, feature_params, lk_params, K, start_idx, new_feature_min_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0dc41",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "- Select two keyframes with large enough baseline\n",
    "- Use indirect (feature-based) or direct (KLT) method to establish keypoint corrispondences between frames\n",
    "- Estimate relative pose and triangulate points to bootstrap point cloud (5-pt RANSAC)\n",
    "- Initialize VO pipeline with inlier keypoints and their associated landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861e643",
   "metadata": {},
   "source": [
    "### Initialization,Feature extraction\n",
    "\n",
    "Detect a set of 2D keypoints in the first bootstrap keyframe (bs_kf_1) that are well distributed across the image.\n",
    "\n",
    "Implementation:\n",
    "- We use `cv2.goodFeaturesToTrack` (Shi–Tomasi) to detect corners.\n",
    "- To avoid clustering of keypoints in high-texture areas, we apply Shi–Tomasi separately in multiple image regions using `params.feature_masks` and then concatenate the results.\n",
    "\n",
    "Output:\n",
    "- `st_corners`: array of shape `(N, 1, 2)` (float32), suitable for KLT tracking with `cv2.calcOpticalFlowPyrLK`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89c7cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(img_grayscale, params):\n",
    "    \"\"\"\n",
    "    Step 1 (Initialization): detect Shi–Tomasi corners on a grid using feature masks.\n",
    "\n",
    "    Args:\n",
    "        img_grayscale (np.ndarray): bootstrap keyframe 1 in grayscale (H x W).\n",
    "        params (VO_Params): contains feature_masks and shi_tomasi_params.\n",
    "\n",
    "    Returns:\n",
    "        st_corners (np.ndarray): (N, 1, 2) float32 corners for KLT tracking.\n",
    "    \"\"\"\n",
    "    st_corners = np.empty((0, 1, 2), dtype=np.float32)\n",
    "    for n, mask in enumerate(params.feature_masks):\n",
    "        features = cv2.goodFeaturesToTrack(img_grayscale, mask=mask, **params.shi_tomasi_params)\n",
    "        # If no corners are found in this region, skip it\n",
    "        if features is None: \n",
    "            print(f\"No features found for mask {n+1}!\")\n",
    "            continue\n",
    "        # Warn if very few features were found in this region (not necessarily an error)\n",
    "        if features.shape[0] < 10:\n",
    "            print(f\"Only {features.shape[0]} features found for mask {n+1}!\")\n",
    "        st_corners = np.vstack((st_corners, features))\n",
    "    return st_corners\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d689b0",
   "metadata": {},
   "source": [
    "### Initialization, KLT tracking across intermediate frames\n",
    "\n",
    "Establish 2D–2D correspondences between the two bootstrap keyframes (bs_kf_1 → bs_kf_2).\n",
    "\n",
    "Implementation:\n",
    "- Tracking the detected keypoints using KLT optical flow (`cv2.calcOpticalFlowPyrLK`).\n",
    "- Instead of tracking directly from keyframe 1 to keyframe 2 in one shot, we track *frame-by-frame* across the intermediate frames,more stable when the motion between keyframes is larger.\n",
    "- We maintain a boolean mask `still_detected` that keeps track of keypoints successfully tracked at every frame.\n",
    "\n",
    "Output:\n",
    "- `points[still_detected]`: tracked keypoints in `bs_kf_2`\n",
    "- `initial_points`: initial deteced keypoints in `bs_kf_1`\n",
    "- `still_detected`: boolean mask (relative to the original set) indicating which keypoints survived the entire bootstrap tracking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c38127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackForwardBootstrap(images,params,st_corners_kf_1):\n",
    "    \"\"\"\n",
    "    Module 2-Initialization: track keypoints from bs_kf_1 to bs_kf_2 with KLT across intermediate frames.\n",
    "\n",
    "    Args:\n",
    "        images (list[str]): full dataset sequence (paths).\n",
    "        params (VO_Params): contains bs_kf_1, bs_kf_2, klt_params.\n",
    "        st_corners_kf_1 (np.ndarray): (N,1,2) keypoints in bs_kf_1.\n",
    "\n",
    "    Returns:\n",
    "        points[still_detected] (np.ndarray): (M,1,2) tracked points in bs_kf_2.\n",
    "        initial_points (np.ndarray): (N,1,2) points in bs_kf_1.\n",
    "        still_detected (np.ndarray): (N,) boolean mask of points visible i all frames.\n",
    "    \"\"\"\n",
    "    img_bs_kf_1_index=images.index(params.bs_kf_1)\n",
    "    img_bs_kf_2_index=images.index(params.bs_kf_2)\n",
    "    still_detected=np.ones(st_corners_kf_1.shape[0],dtype=bool)\n",
    "    points = st_corners_kf_1.copy()\n",
    "    initial_points = st_corners_kf_1.copy()\n",
    "    #Track keypoints frame-by-frame from first bs frame to second bs frame\n",
    "    for i in range(img_bs_kf_1_index, img_bs_kf_2_index):\n",
    "        current_image=cv2.imread(images[i],cv2.IMREAD_GRAYSCALE)\n",
    "        next_image=cv2.imread(images[i+1],cv2.IMREAD_GRAYSCALE)\n",
    "        nextPts,status,error=cv2.calcOpticalFlowPyrLK(current_image,next_image,points, None, **params.klt_params)\n",
    "        points=nextPts\n",
    "        status=status.flatten()\n",
    "        still_detected=still_detected & (status==1)\n",
    "\n",
    "    # Keep only points that were successfully tracked throughout\n",
    "    return points[still_detected], initial_points, still_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a8fa2",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "TODO: find transformation between the two frames using cv2.findHomography()\n",
    "\n",
    "### Initialization, Relative pose estimation with RANSAC\n",
    "\n",
    "Estimate the relative camera motion between the two bootstrap keyframes using\n",
    "2D–2D correspondences obtained after KLT tracking.\n",
    "\n",
    "Since the tracked correspondences still contain outliers, we  estimate the epipolar geometry using RANSAC.\n",
    "\n",
    "Procedure:\n",
    "1. Estimate the Fundamental matrix using `cv2.findFundamentalMat` with RANSAC.\n",
    "2. Convert the Fundamental matrix into the Essential matrix using camera intrinsics.\n",
    "3. Recover the relative rotation and translation (up to scale) using `cv2.recoverPose`.\n",
    "4. Keep only inlier correspondences.\n",
    "\n",
    "The resulting relative pose (R, t) is returned together with inliers bollean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd1453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransacHomography(points1,points2):\n",
    "    \"\"\"\n",
    "    Estimate relative pose between two keyframes using RANSAC.\n",
    "\n",
    "    Args:\n",
    "        points1, points2: corresponding 2D points (N,2) or (N,1,2)\n",
    "\n",
    "    Returns:\n",
    "        R (3x3): relative rotation matrix\n",
    "        t (3x1): relative translation vector (up to scale)\n",
    "        inliers (N,): boolean mask of inlier correspondences\n",
    "    \"\"\"\n",
    "    #F mat using ransac\n",
    "    fundamental_matrix, inliers =cv2.findFundamentalMat(points1,points2,cv2.FM_RANSAC,ransacReprojThreshold=1.0)\n",
    "\n",
    "    #using boolean vector\n",
    "    inliers = inliers.ravel().astype(bool)\n",
    "\n",
    "    #compute the essential matrix\n",
    "    E= K.T@ fundamental_matrix@K\n",
    "\n",
    "    #recover the relative camera pose\n",
    "    _,R,t,mask_pose=cv2.recoverPose(E,points1[inliers],points2[inliers],K)\n",
    "\n",
    "    return R, t, inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check point 1 and point 2 and point 3\n",
    "#read bootstrap keyframes\n",
    "img_bs_kf_1 = cv2.imread(params.bs_kf_1,cv2.IMREAD_GRAYSCALE)\n",
    "img_bs_kf_2 = cv2.imread(params.bs_kf_2,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#1)extract features\n",
    "st_corners_kf_1 = extract_features(img_bs_kf_1, params)\n",
    "\n",
    "#2)KLT interframe tracking\n",
    "st_corners_kf_2, st_corners_kf_1,still_detected =feature_tracking_klt(images, params, st_corners_kf_1)\n",
    "\n",
    "print(\"Initial corners:\",st_corners_kf_1.shape[0])\n",
    "print(\"Tracked corners:\",st_corners_kf_2.shape[0])\n",
    "\n",
    "\n",
    "# 3)estimate relative pose with RANSAC\n",
    "R, t, inliers = estimate_relative_pose(st_corners_kf_1[still_detected], st_corners_kf_2)\n",
    "\n",
    "print(\"Inliers after RANSAC:\", inliers.sum(), \"/\", len(inliers))\n",
    "print(\"R:\\n\", R)\n",
    "print(\"t (up to scale):\\n\", t.T)\n",
    "\n",
    "# Visual test: inliers (green) vs outliers (red) on keyframe 1\n",
    "vis = cv2.cvtColor(img_bs_kf_1, cv2.COLOR_GRAY2BGR)\n",
    "pts1 = st_corners_kf_1[still_detected].reshape(-1, 2)\n",
    "\n",
    "for i, (x, y) in enumerate(pts1):\n",
    "    color = (0, 255, 0) if inliers[i] else (0, 0, 255)\n",
    "    cv2.circle(vis, (int(x), int(y)), 2, color, -1)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(vis[..., ::-1])  # BGR -> RGB\n",
    "plt.title(\"Step3 RANSAC: inliers (green) / outliers (red)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ecc4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inlier ratio:\", inliers.sum() / len(inliers))\n",
    "print(\"||t||:\", np.linalg.norm(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07983c3e",
   "metadata": {},
   "source": [
    "### Triangulate points to get point cloud\n",
    "TODO: find 3d points using triangulatePoints(); the projection matrices are $K*[R|T]$ where $[R|T]$ is $[I|0]$ for the first image and the homography from above for the second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ae1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapPointCloud(params: VO_Params, H: np.ndarray, points_1: np.ndarray, points_2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Bootstrap the initial 3D point cloud using least squares assuming the first frame is the origin\n",
    "\n",
    "    Args:\n",
    "        params (VO_Params): params object for the dataset being used\n",
    "        H (np.ndarray): homographic transformation from bootstrap keyframe 1 to 2\n",
    "        points_1 (np.ndarray): keypoints detected in bootstrap keyframe 1\n",
    "        points_2 (np.ndarray): keypoints tracked in bootstrap keyframe 2\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: [3 x k] array of triangulated points\n",
    "    \"\"\"\n",
    "    # projection matrices\n",
    "    proj_1 = params.k @ np.hstack([np.eye(3), np.zeros((3,1))])\n",
    "    proj_2 = params.k @ H\n",
    "\n",
    "    # triangulate homogeneous coordinates using DLT\n",
    "    points_homo = cv2.triangulatePoints(proj_1, proj_2, points_1, points_2)\n",
    "\n",
    "    # convert back to 3D\n",
    "    points_3d = (points_homo[:3, :]/points_homo[3, :])\n",
    "\n",
    "    return points_3d\n",
    "#pts = bootstrapPointCloud(params, np.hstack([np.eye(3), np.ones((3,1))]), H_bootstrap, st_corners_kf_1, st_corners_kf_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08c00f",
   "metadata": {},
   "source": [
    "### Build state to initialize the algorithm\n",
    "The state is $(P^i, X^i, C^i, F^i, \\Tau^i)$ where<br>\n",
    "$P^i$ is a `[k x 1 x 2]` matrix of initial features' pixel in the second keyframe of the dataset<br>\n",
    "$X^i$ is a `[3 x k]` matrix of the 3D cooridinates of the corrisponding landmarks<br>\n",
    "$C^i$ is a `[m x 1 x 2]` matrix of current locations of candidate keypoints (empty to start so `m=0`)<br>\n",
    "$F^i$ is a `[m x 1 x 2]` matrix of initial observation of candidate keypoints (empty to start so `m=0`)<br>\n",
    "$\\Tau^i$ is a `[m x 12]` matrix of the camera pose during the initial observation (empty at the start so `m=0`)<br>\n",
    "This can be stored in a dict $S = \\{P: [k \\times 1 \\times 2], X: [3 \\times k], ...\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapState(P_i: np.ndarray, X_i: np.ndarray) -> dict[str : np.ndarray]:\n",
    "    \"\"\"\n",
    "        Builds the initial state taking the 2D keypoints and their respective 3D points generated by the bootstrap\n",
    "        \n",
    "        Args:\n",
    "            P_i: kx1x2 matrix of 2D keypoints found from the first two frames\n",
    "            X_i: 3xk matrix containing the 3D projections of the keypoints\n",
    "        \n",
    "        Returns:\n",
    "            dictionary of dictionaries, where every variable of interest is a key and its value is the matrix (e.g. S[\"P\"] returns a kx1x2 matrix of keypoints)  \n",
    "    \"\"\"\n",
    "    S : dict[str : np.ndarray] = {}\n",
    "    assert P_i.shape()[0] == X_i.shape()[1], \"2D keypoints number of rows should match the 3D keypoints number of columns\"\n",
    "    \n",
    "    S[\"P\"] = P_i    \n",
    "    S[\"X\"] = X_i    \n",
    "    S[\"C\"] = np.empty(0,1,2)\n",
    "    S[\"F\"] = np.empty_like(S[\"C\"])\n",
    "    S[\"T\"] = np.empty(0,12)\n",
    "\n",
    "    return S\n",
    "\n",
    "# init_state = bootstrapState(P_i = st_corners_kf_1, X_i = pts) # might be wrong, double check on sunday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f86f87",
   "metadata": {},
   "source": [
    "# Operation\n",
    "- Match keypoints in current image to existing landmarks\n",
    "    - Extract keypoints (Harris)\n",
    "    - Track (KLT)\n",
    "- Estimate pose\n",
    "    - Estimate pose and handle outliers (P3P plus RANSAC)\n",
    "- Add new landmarks as needed by triangulating new features\n",
    "    - Keep track of candidate landmarks\n",
    "        - Keypoint itself\n",
    "        - Observation when first seen\n",
    "        - Pose when first seen\n",
    "    - Only add when they have been tracked for long enough and baselineis large enough\n",
    "    - Discard if track fails<br>\n",
    "\n",
    "_NOTE: this starts at the frame after the second keyframe (`bs_kf_2`) and goes until the last frame in the dataset_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20de2d8",
   "metadata": {},
   "source": [
    "### Track keypoints forward one frame\n",
    "Here we want to use cv2.calcOpticalFlowPyrLK() with previous frame, current frame, $P^i$ and $C^i$,  - see use in initialization for example and then update $P^i$ and $X^i$ as well as $C^i$, $F^i$ and $\\Tau^i$ based on the features that were successfully tracked (ie remove any features that were not tracked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac891bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackForward(params: VO_Params, state: dict[str:np.ndarray], img_1: np.ndarray, img_2: np.ndarray) -> dict[str:np.ndarray]:\n",
    "    \"\"\"\n",
    "    Track 2D keypoints from img_1 to img_2 using KLT optical flow\n",
    "\n",
    "    Args:\n",
    "        params (VO_Params): parameters for the VO pipeline\n",
    "        img_1 (np.ndarray): first image (grayscale)\n",
    "        img_2 (np.ndarray): second image (grayscale)\n",
    "        points_1 (np.ndarray): keypoints in img_1 to be tracked\n",
    "    Returns:\n",
    "        np.ndarray: tracked keypoints in img_2\n",
    "    \"\"\"\n",
    "    # NOTE: might make sense to replace assert with ifs since maybe even if we have no points nor candidates we still want to return them empty again\n",
    "    # FIRST WE TRACK \"ESTABILISHED KEYPOINTS\"\n",
    "    points_2D = state[\"P\"]\n",
    "    assert points_2D.shape[0] > 0, \"There are no keypoints here, we can't track them forward\"    \n",
    "\n",
    "    current_points, status, _ = cv2.calcOpticalFlowPyrLK(prevImg=img_1, nextImg=img_2, prevPts=points_2D, nextPts=None, **params.klt_params)\n",
    "    status = status.flatten()   # we are going to use them as booleans so maybe we should cast them with astype (?)\n",
    "    \n",
    "    # update the state with the current points\n",
    "    state[\"P\"] = current_points[status]\n",
    "    state[\"X\"] = state[\"X\"][:, status]  # only get the ones with \"true\" status and slice them as 3xk\n",
    "    \n",
    "\n",
    "    # THEN WE TRACK CANDIDATES\n",
    "    candidates = state[\"C\"]\n",
    "    assert candidates.shape[0] > 0, \"No candidates found, something went wrong\"\n",
    "    \n",
    "    current_cands, status_cands, _ = cv2.calcOpticalFlowPyrLK(prevImg=img_1, nextImg=img_2, prevPts=points_2D, nextPts=None, **params.klt_params)\n",
    "    status_cands = status.flatten() # same as above\n",
    "    \n",
    "    state[\"C\"] = current_cands[status_cands]\n",
    "    # initial observations, still have some doubts on these two\n",
    "    state[\"F\"] = state[\"F\"][status_cands]\n",
    "    state[\"T\"] = state[\"T\"][status_cands]\n",
    "\n",
    "    return state      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d629eebb",
   "metadata": {},
   "source": [
    "### Estimate pose\n",
    "Here we want to use cv2.solvePnPRansac() with updated $P^i$, $X^i$, $K$, to find pose of camera at current position and then update $P^i$ and $X^i$ with the inliers at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimatePose(params: VO_Params, state: dict[str:np.ndarray]) -> tuple[dict[str:np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Estimate camera pose using PnP RANSAC and update state to keep only inliers\n",
    "    \n",
    "    Args:\n",
    "        params (VO_Params): parameters for the VO pipeline\n",
    "        state (dict): current state that also contains 2D keypoints and 3D points\n",
    "    \n",
    "    Returns:\n",
    "        tuple[dict, np.ndarray]: updated state with only inliers for P and X and camera pose as 3x4 matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    pts_2D = state[\"P\"]\n",
    "    pts_3D = state[\"X\"]\n",
    "    pts_3D = pts_3D.T # according to documentation we need them as kx3 not 3xk\n",
    "    K = params.k\n",
    "    \n",
    "    success, r_vec, t_vec, inliers_idx =  cv2.solvePnPRansac(objectPoints=pts_3D, imagePoints=pts_2D, cameraMatrix=K, distCoeffs=None, flags=cv2.SOLVEPNP_P3P)  # using P3P as suggested in the statement\n",
    "    \n",
    "    if not success:\n",
    "        print(\"Pose estimation failed\")\n",
    "        return (np.empty(), {}) # maybe we could raise an error instead of returning this\n",
    "    \n",
    "    # r_vec needs to be converted into a 3x3\n",
    "    R, _ = cv2.Rodrigues(r_vec)    \n",
    "    camera_pose = np.hstack(R, t_vec)\n",
    "    \n",
    "    # now, inliers are the indices in pts_2d and pts_3d corresponding to the inliers; it is a 2D since openCV returns it as such, so we need to convert it in a 1D array to use np features\n",
    "    inliers_idx = inliers_idx.flatten()\n",
    "    new_state = state.copy()\n",
    "    # by doing the following thing the idea is that we are only keeping the inliers\n",
    "    new_state[\"P\"] = state[\"P\"][inliers_idx]\n",
    "    new_state[\"X\"] = state[\"X\"][:, inliers_idx] # slice this since we want it as a 3xk\n",
    "    \n",
    "    return (new_state, camera_pose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02901b0f",
   "metadata": {},
   "source": [
    "### Try triangulating\n",
    "TODO: Check the angle between the the first observation ($X^i$ and $\\Tau^i$) of candidate keypoints and the current observation ($C^i$ and current pose) - triangulate features using cv2.triangulatePoints() if angle is above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61945b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4a2379b",
   "metadata": {},
   "source": [
    "### Find new features\n",
    "TODO: use cv2.goodFeaturesToTrack() - see use in initialization for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f162ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tstt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f324c98",
   "metadata": {},
   "source": [
    "### Add new features\n",
    "TODO: if the feature is not already being tracked as an actual keypoint or candidate keypoint, update $C^i$ and $X^i$ with keypoint location and $\\Tau^i$ with the current camera pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints2set(keypoints: np.ndarray) -> set:\n",
    "    \"\"\"Convert numpy keypoint list [k x 1 x 2] to a set of keypoints\n",
    "\n",
    "    Args:\n",
    "        keypoints (np.ndarray): keypoint array\n",
    "\n",
    "    Returns:\n",
    "        set: keypoint set\n",
    "    \"\"\"\n",
    "    return set([(row[0][0], row[0][1]) for row in keypoints.tolist()])\n",
    "\n",
    "def set2keypoints(keypoint_set: set) -> np.ndarray:\n",
    "    \"\"\"Convert keypoint set (u, v) to a numpy array [k x 1 x 2]\n",
    "\n",
    "    Args:\n",
    "        keypoint_set (set): keypoint set\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: keypoint array with shape [k x 1 x 2]\n",
    "    \"\"\"\n",
    "    return np.array([[[keypoint[0], keypoint[1]]] for keypoint in keypoint_set])\n",
    "\n",
    "def addNewFeatures(params: VO_Params, S: dict, potential_candidate_features: np.ndarray, cur_pose: np.ndarray) -> dict:\n",
    "    \"\"\"Given an array of features, update S with featurees that are not already being tracked\n",
    "\n",
    "    Args:\n",
    "        params (VO_Params): params object for the dataset being used\n",
    "        S (dict): state\n",
    "        potential_candidate_features (np.ndarray): features extracted from current frame\n",
    "        cur_pose (np.ndarray): pose of current frame\n",
    "\n",
    "    Returns:\n",
    "        dict: updated state\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    cur = np.vstack((S[\"P\"], S[\"C\"]))\n",
    "    new = potential_candidate_features\n",
    "    n_new = new.shape[0]\n",
    "    n_cur = cur.shape[0]\n",
    "\n",
    "    # extend new features to a tensor where every row corrisponds to a new feature,\n",
    "    # every column will be used to compare to existing features, and depth holds the x, y coords\n",
    "    new_extended = new.repeat(n_cur, axis=1)\n",
    "\n",
    "    # reshape current features so it can be subtracted from every column in the tensor of potential new points\n",
    "    cur_reshaped = cur.reshape((1, n_cur, 2))\n",
    "\n",
    "    # calculate the distances between every point pair (rows corrispond to new features, cols to cur features)\n",
    "    dists = np.linalg.norm(new_extended - cur_reshaped, axis=2)\n",
    "\n",
    "    # for every new point, find the distance to the closest current point\n",
    "    min_dists = np.max(dists, axis=1)\n",
    "\n",
    "    # create a mask, keeping only features that are greater than a minimum distance from any already tracked feature\n",
    "    new_features_mask = np.where(min_dists > params.new_feature_min_dist, True, False)\n",
    "\n",
    "    # mask the potential new features so only unique ones are kept\n",
    "    new_features = new[new_features_mask]\n",
    "\n",
    "    # append new features to current points, first observed points, and first observed camera pose\n",
    "    S[\"C\"] = np.vstack((S[\"C\"], new_features))\n",
    "    S[\"X\"] = np.vstack((S[\"X\"], new_features))\n",
    "    S[\"T\"] = np.vstack(S[\"T\"], cur_pose.flatten()[None, :].repeat(len(new_features)))\n",
    "\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7546e",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "_Thanks to our good friend ChatGPT_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "def initTrajectoryPlot(gt_path: np.ndarray, arrow_len: float = 0.3) -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Initialize trajectory plot with ground truth and placeholders\n",
    "    for estimated trajectory and camera orientation.\n",
    "\n",
    "    Args:\n",
    "        gt_path (np.ndarray): Ground truth path [n x 2]\n",
    "        arrow_len (float): Length of orientation arrow (meters)\n",
    "\n",
    "    Returns:\n",
    "        dict: plot state dictionary\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    if DATASET in [0, 2]:\n",
    "        # ---- Ground truth (time-colored) ----\n",
    "        x_gt, y_gt = gt_path[:, 0], gt_path[:, 1]\n",
    "        t = np.arange(len(gt_path))\n",
    "\n",
    "        points = np.stack((x_gt, y_gt), axis=1).reshape(-1, 1, 2)\n",
    "        segments = np.concatenate((points[:-1], points[1:]), axis=1)\n",
    "\n",
    "        norm = mpl.colors.Normalize(vmin=t.min(), vmax=t.max())\n",
    "        lc = LineCollection(segments, cmap=\"viridis\", norm=norm, linewidth=2.5)\n",
    "        lc.set_array(t)\n",
    "\n",
    "        ax.add_collection(lc)\n",
    "        cbar = fig.colorbar(lc, ax=ax)\n",
    "        cbar.set_label(\"Time step (GT)\")\n",
    "\n",
    "    # ---- Estimated trajectory ----\n",
    "    est_line, = ax.plot([], [], \"r-\", linewidth=2, label=\"VO estimate\")\n",
    "    est_point, = ax.plot([], [], \"ro\", markersize=5)\n",
    "\n",
    "    # ---- Orientation arrow ----\n",
    "    heading_arrow = FancyArrowPatch(\n",
    "        (0.0, 0.0),\n",
    "        (0.0, 0.0),\n",
    "        arrowstyle=\"->\",\n",
    "        linewidth=2,\n",
    "        color=\"red\",\n",
    "        mutation_scale=15,\n",
    "    )\n",
    "    ax.add_patch(heading_arrow)\n",
    "\n",
    "    # ---- Formatting ----\n",
    "    ax.set_title(\"Ground Truth vs VO Estimated Trajectory\")\n",
    "    ax.set_xlabel(\"x [m]\")\n",
    "    ax.set_ylabel(\"y [m]\")\n",
    "    ax.axis(\"equal\")\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.autoscale()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"fig\": fig,\n",
    "        \"ax\": ax,\n",
    "        \"est_line\": est_line,\n",
    "        \"est_point\": est_point,\n",
    "        \"heading_arrow\": heading_arrow,\n",
    "        \"arrow_len\": arrow_len,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf012e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTrajectoryPlot(plot_state: Dict[str, object], est_path: np.ndarray, theta: float) -> None:\n",
    "    \"\"\"\n",
    "    Update estimated trajectory and camera orientation arrow.\n",
    "\n",
    "    Args:\n",
    "        plot_state (dict): Plot state returned by initTrajectoryPlot\n",
    "        est_path (np.ndarray): Estimated path [k x 2]\n",
    "        theta (float): Current yaw angle (radians)\n",
    "    \"\"\"\n",
    "    x = est_path[:, 0]\n",
    "    y = est_path[:, 1]\n",
    "\n",
    "    # Update trajectory\n",
    "    plot_state[\"est_line\"].set_data(x, y)\n",
    "    plot_state[\"est_point\"].set_data(x[-1], y[-1])\n",
    "\n",
    "    # Update orientation arrow\n",
    "    x0, y0 = x[-1], y[-1]\n",
    "    L = plot_state[\"arrow_len\"]\n",
    "    x1 = x0 + L * np.cos(theta)\n",
    "    y1 = y0 + L * np.sin(theta)\n",
    "\n",
    "    plot_state[\"heading_arrow\"].set_positions((x0, y0), (x1, y1))\n",
    "\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca661d",
   "metadata": {},
   "source": [
    "# Real-Time Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdccd65",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89421f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from the first image of the dataset\n",
    "bootstrap_features_kf_1 = extractFeatures(params, params.bs_kf_1)\n",
    "\n",
    "# track extracted features forward to the next keyframe in the dataset\n",
    "bootstrap_tracked_features_kf_1, bootstrap_tracked_features_kf_2 = trackForwardBootstrap(params, bootstrap_features_kf_1)\n",
    "\n",
    "# calculate the homographic transformation between the first two keyframes\n",
    "homography, ransac_features_kf_1, ransac_features_kf_2 = ransacHomography(params, bootstrap_tracked_features_kf_1, bootstrap_tracked_features_kf_2)\n",
    "\n",
    "# triangulate features from the first two keyframes to generate initial 3D point cloud\n",
    "bootstrap_point_cloud = bootstrapPointCloud(params, homography, ransac_features_kf_1, bootstrap_tracked_features_kf_2)\n",
    "\n",
    "# generate initial state\n",
    "S = bootstrapState(P_i=bootstrap_tracked_features_kf_2, X_i=bootstrap_point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708d031",
   "metadata": {},
   "source": [
    "## Looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting setup\n",
    "plot_state = initTrajectoryPlot(ground_truth)\n",
    "est_path = []\n",
    "theta = 0.0\n",
    "\n",
    "# initialize previous image\n",
    "last_image = cv2.imread(images[params.start_idx], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "for i in range(params.start_idx + 1, last_frame + 1):\n",
    "\n",
    "    # read in next image\n",
    "    image_path = images[i]\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"Warning: could not read {image_path}\")\n",
    "        continue\n",
    "\n",
    "    # track keypoints forward one frame\n",
    "    S = trackForward(params, S, last_image, image)\n",
    "\n",
    "    # estimate pose, only keeping inliers from PnP with RANSAC\n",
    "    S, pose = estimatePose(params, S)\n",
    "\n",
    "    # attempt triangulating candidate keypoints, only adding ones with sufficient baseline\n",
    "    S = attemptTriangulating(params, S, pose)\n",
    "\n",
    "    # find features in current frame\n",
    "    potential_candidate_features = extractFeatures(params, image)\n",
    "\n",
    "    # find which features are not currently tracked and add them as candidate features\n",
    "    S = addNewFeatures(params, S, potential_candidate_features, pose)\n",
    "\n",
    "    # plot current pose\n",
    "    est_path.append(pose[:2, 3])\n",
    "    theta = scipy.spatial.transform.Rotation.from_matrix(pose[:3, :3]).as_euler(\"xyz\")[2]\n",
    "    updateTrajectoryPlot(plot_state, np.asarray(est_path), theta)\n",
    "\n",
    "    # update last image\n",
    "    last_image = image\n",
    "\n",
    "    # pause for 0.01 seconds\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
